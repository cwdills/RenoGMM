{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Python Notebook to Test Openquake and Pygmm\n",
    "# Daniel Trugman, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from obspy.clients.fdsn import Client\n",
    "import rasterio\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#sns.set_style(\"darkgrid\")\n",
    "\n",
    "# openquake - generic\n",
    "# - https://docs.openquake.org/oq-engine/3.5/baselib.html#installation\n",
    "#  - worked ok as !pip install openquake.engine in python 3.10 environment...\n",
    "#    - but needed to add fiona afterward with mamba\n",
    "# - for documentation see this:\n",
    "# - https://docs.openquake.org/oq-engine/3.5/openquake.hazardlib.html\n",
    "from openquake.hazardlib.contexts import RuptureContext\n",
    "from openquake.hazardlib.contexts import DistancesContext\n",
    "from openquake.hazardlib.contexts import SitesContext\n",
    "from openquake.hazardlib import imt, const\n",
    "\n",
    "# active crustal models\n",
    "from openquake.hazardlib.gsim.abrahamson_2014 import AbrahamsonEtAl2014\n",
    "from openquake.hazardlib.gsim.boore_2014 import BooreEtAl2014\n",
    "from openquake.hazardlib.gsim.campbell_bozorgnia_2014 import CampbellBozorgnia2014\n",
    "from openquake.hazardlib.gsim.chiou_youngs_2014 import ChiouYoungs2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "\n",
    "## Define Rupture Context\n",
    "rctx = RuptureContext()\n",
    "rctx.mag = 5.7\n",
    "rctx.rake = -19\n",
    "rctx.dip = 80\n",
    "rctx.ztor = 0.1 #guessing\n",
    "rctx.width = 10 # guessing\n",
    "rctx.hypo_depth = 9.3 \n",
    "\n",
    "## Define Distance Context (assume fault-perpendicular sites)\n",
    "dctx = DistancesContext()\n",
    "rjbs = np.linspace(0.0, 320.0, 321) # rjb values in km\n",
    "rrups = np.sqrt(rjbs**2 + rctx.ztor**2) # calculated rupture distance for a vertical fault\n",
    "npts = rrups.size # number of distance points\n",
    "dctx.rrup = rrups # Closest distance to rupture surface\n",
    "dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                         #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "\n",
    "\n",
    "## Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "#    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                }\n",
    "sitecollection = pd.DataFrame(sitecol_dict)\n",
    "sctx = SitesContext(sitecol=sitecollection)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Calculate Ground Motions\n",
    "\n",
    "# define metrics\n",
    "#IMT = imt.PGA()\n",
    "IMT = imt.PGV()\n",
    "#IMT = imt.SA(period=0.1)\n",
    "uncertaintytype = const.StdDev.TOTAL\n",
    "\n",
    "# instantiate GMMs\n",
    "ASK14 = AbrahamsonEtAl2014()\n",
    "BSSA14 = BooreEtAl2014()\n",
    "CB14 = CampbellBozorgnia2014()\n",
    "CY14 = ChiouYoungs2014()\n",
    "\n",
    "# calculate ln ground motions for each\n",
    "ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "\n",
    "# average across all four for active crustal values\n",
    "ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Find Observed Data\n",
    "\n",
    "df = pd.read_csv('nn00888580_default_metrics_rotd(percentile=50.0).csv')\n",
    "OBSdrup = df['RuptureDistance']\n",
    "OBSpga = df['PGA']\n",
    "OBSpgv = df['PGV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Plot Model Results\n",
    "\n",
    "# figure setup\n",
    "fig, axi = plt.subplots(figsize=(6,4))\n",
    "axi.set_facecolor(\"gainsboro\")\n",
    "\n",
    "# plot GMPE\n",
    "ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "plt.scatter(OBSdrup, OBSpgv, s=5, label=\"Parker Butte M5.7 Observed Motions\", zorder=0) #OBSpgv or OBSpga\n",
    "axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "#axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "#axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "\n",
    "# formatting\n",
    "axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "axi.grid(lw=0.3)\n",
    "axi.legend(loc=\"upper right\",fontsize=11)\n",
    "axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show results\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- basin term and vs30 defaults\n",
    "- optimize rupture context\n",
    "- optimize distance metrics?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 45\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30':760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpgv\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(totalresiduals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGA)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 90\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    #IMT = imt.PGV()\n",
    "    IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSsa\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGA)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 90\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    #IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpga\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (PLOTTING)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth'] \n",
    "\n",
    "    ## Define Rupture Context\n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude #PB 5.7\n",
    "    rctx.rake = 0 #PB -19\n",
    "    rctx.dip = 90 #PB 80\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km\n",
    "    \n",
    "    rctx.ztor = depth-L/2 #guessing\n",
    "    rctx.width =L # guessing\n",
    "    rctx.hypo_depth = depth #PB 9.3 \n",
    "    \n",
    "    ## Define Distance Context (assume fault-perpendicular sites)\n",
    "    dctx = DistancesContext()\n",
    "    rjbs = np.linspace(0.0, 320.0, 321) # rjb values in km\n",
    "    rrups = np.sqrt(rjbs**2 + rctx.ztor**2) # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    \n",
    "    ## Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA()\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1\n",
    "    \n",
    "    ### Find Observed Data\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"     #FIX THIS WITH EVENT DIRECTORY STUFF FROM ANALYZER\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA'] #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    \n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, OBSpgv, s=5, label=f\"M{magnitude} {event} Observed Motions\", zorder=0) #OBSpgv or OBSpga\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    #axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show results\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30':760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "# Prepare results\n",
    "vs30_values = []\n",
    "measured_flags = []\n",
    "\n",
    "# Iterate over stations\n",
    "for _, station in df.iterrows():\n",
    "    lat = station['StationLatitude']\n",
    "    lon = station['StationLongitude']\n",
    "    \n",
    "    # Query tree for neighbors within the threshold\n",
    "    distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "    index = indices[0]\n",
    "    \n",
    "    if index != tree.n:  # Found a valid neighbor\n",
    "        vs30 = vs30_df.iloc[index]['Vs30']\n",
    "        measured = True\n",
    "    else:\n",
    "        # Fallback to raster\n",
    "        row, col = raster.index(lon, lat)\n",
    "        if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "            vs30 = raster_array[row, col]\n",
    "        else:\n",
    "            vs30 = np.nan\n",
    "        measured = False\n",
    "    \n",
    "    vs30_values.append(vs30)\n",
    "    measured_flags.append(measured)\n",
    "\n",
    "# Add columns to stations DataFrame\n",
    "df['Vs30'] = vs30_values\n",
    "df['measured'] = measured_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV, Vs30 colorbar, can use time subset)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    #Comment this section out if you just want to use whole dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "            #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "        IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            \n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            \n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            \n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['seismic']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=vs30_values, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='Vs30')\n",
    "\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print('Out of Date Range')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "pdf_pages.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV, SNR colorbar, can use time or mag subset)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3\n",
    "    maxmag = 5.7\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].max())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='Vs30')\n",
    "\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print('Outside magnitude')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "pdf_pages.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SNR Analysis\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "        \n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 45\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    \n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    # Prepare results\n",
    "    vs30_values = []\n",
    "    measured_flags = []\n",
    "\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    # Iterate over stations\n",
    "    for _, station in df.iterrows():\n",
    "        lat = station['StationLatitude']\n",
    "        lon = station['StationLongitude']\n",
    "        # Query tree for neighbors within the threshold\n",
    "        distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "        index = indices[0]\n",
    "        if index != tree.n:  # Found a valid neighbor\n",
    "            vs30 = vs30_df.iloc[index]['Vs30']\n",
    "            measured = True\n",
    "        else:\n",
    "            # Fallback to raster\n",
    "            row, col = raster.index(lon, lat)\n",
    "            if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                vs30 = raster_array[row, col]\n",
    "            else:\n",
    "                vs30 = np.nan\n",
    "            measured = False\n",
    "        vs30_values.append(vs30)\n",
    "        measured_flags.append(measured)\n",
    "    \n",
    "    # Add columns to stations DataFrame\n",
    "    df['Vs30'] = vs30_values\n",
    "    df['measured'] = measured_flags\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpgv\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    df['Residuals']= residuals\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    # (optional) find SNR values\n",
    "    snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "    # Prepare list to store the minimum SNRs for each StationID\n",
    "    min_snrs = []\n",
    "    # Iterate through each StationID\n",
    "    for station_id in df['StationID']:\n",
    "        station_id = str(station_id)  # Make sure it's a string\n",
    "        # Update count for this StationID\n",
    "        station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "        # Find rows in snr_df where TraceID starts with the StationID\n",
    "        matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "    \n",
    "        if not matches.empty:\n",
    "            min_snr = matches['SNR(1)'].max()\n",
    "        else:\n",
    "            min_snr = float('0') # Or some default, like None or 0\n",
    "            print('Did not find stationID in SNR csv')\n",
    "        min_snrs.append(min_snr)\n",
    "    \n",
    "    # Add the result to the original DataFrame\n",
    "    df['MinSNR'] = min_snrs\n",
    "    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "    \n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "     # Create a colormap\n",
    "    cmap = plt.colormaps['Reds']\n",
    "    # Normalize the values to the range 0-1 for the colormap\n",
    "    #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "    #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "    norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "    #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.colorbar(label='SNR')\n",
    "\n",
    "    #pdf_pages.savefig(fig)\n",
    "    #plt.close(fig)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SNR Analysis (Adding magnitude)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3\n",
    "    maxmag = 5.7\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            station_id = str(station_id)  # Make sure it's a string\n",
    "            # Update count for this StationID\n",
    "            station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "        norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='SNR')\n",
    "    \n",
    "        #pdf_pages.savefig(fig)\n",
    "        #plt.close(fig)\n",
    "    else:\n",
    "        print('Out of magnitude range')\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "totalmagnitudes = []\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole timeline\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3.5\n",
    "    maxmag = 4\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        totalmagnitudes.append(magnitude)\n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 10*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            station_id = str(station_id)  # Make sure it's a string\n",
    "            # Update count for this StationID\n",
    "            station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "        norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='SNR')\n",
    "    \n",
    "        #pdf_pages.savefig(fig)\n",
    "        #plt.close(fig)\n",
    "    else:\n",
    "        print('Out of magnitude range')\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "totalmagnitudes = []\n",
    "totaldistances = []\n",
    "totalstationresiduals = []\n",
    "allSNRrecordings = pd.DataFrame()\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"ParkerButteTestCSV.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    #Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2000-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2025-06-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:\n",
    "\n",
    "        mag = row['EarthquakeMagnitude']\n",
    "        minmag = 2.5\n",
    "        maxmag = 5.7\n",
    "        # Check if mag is between min and max\n",
    "        if minmag <= mag <= maxmag:\n",
    "            \n",
    "            event = row['eventID']\n",
    "            magnitude = row['EarthquakeMagnitude']\n",
    "            depth = row['EarthquakeDepth']\n",
    "            totalmagnitudes.append(magnitude)\n",
    "            # download more rupture data for the event\n",
    "            cat = client.get_events(eventid=f\"{event}\")\n",
    "            cat = cat[0]\n",
    "            \n",
    "            #find event specific data from gmprocess (Reno1)\n",
    "            base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "            filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "            file_path = os.path.join(base_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "                df = df.reset_index(drop=True)\n",
    "\n",
    "            ### Determine Magnitude Type\n",
    "            try:\n",
    "                value = df.iat[1, 6]\n",
    "                if pd.notna(value):\n",
    "                    magnitudetype = value.strip().lower()\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except (IndexError, ValueError, AttributeError):\n",
    "                print(f\"Didn't find magnitudetype for {event}\")\n",
    "                magnitudetype = \"mw\"\n",
    "\n",
    "            if magnitudetype != \"mw\":\n",
    "                try:\n",
    "                    nil_df = pd.read_csv(f'/Users/carterdills/Documents/nsl_webmt.csv')\n",
    "                    event_suffix = str(event)[-6:]\n",
    "                    if event_suffix in nil_df.iloc[:, 0].astype(str).values:\n",
    "                        print(f\"Replaced {magnitudetype} with mw for {event}\")\n",
    "                        magnitudetype = \"mw\"\n",
    "                    else:\n",
    "                        print(f\"Converted {magnitudetype} for {event} mathematically\")\n",
    "\n",
    "                        x = magnitude  # store current value\n",
    "                        if magnitudetype == \"ml\":\n",
    "                            if 3.5 <= x <= 7:\n",
    "                                magnitude = 2.29155 + 0.02207 * x + 0.09782 * x**2\n",
    "                            elif x < 3.5:\n",
    "                                magnitude = 1.09329 + 0.70679 * x\n",
    "                            else:\n",
    "                                print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                        elif magnitudetype == \"md\":\n",
    "                            if 3.5 <= x <= 7:\n",
    "                                magnitude = 2.34570 + 0.00448 * x + 0.10308 * x**2\n",
    "                            elif x < 3.5:\n",
    "                                magnitude = 1.08291 + 0.72608 * x\n",
    "                            else:\n",
    "                                print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading nil_webmt.csv or processing event {event}: {e}\")\n",
    "                    print(f\"Assuming mathematical conversion for {magnitudetype} on {event}\")\n",
    "                    \n",
    "                    x = magnitude\n",
    "                    if magnitudetype == \"ml\":\n",
    "                        if 3.5 <= x <= 7:\n",
    "                            magnitude = 2.29155 + 0.02207 * x + 0.09782 * x**2\n",
    "                        elif x < 3.5:\n",
    "                            magnitude = 1.09329 + 0.70679 * x\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                    elif magnitudetype == \"md\":\n",
    "                        if 3.5 <= x <= 7:\n",
    "                            magnitude = 2.34570 + 0.00448 * x + 0.10308 * x**2\n",
    "                        elif x < 3.5:\n",
    "                            magnitude = 1.08291 + 0.72608 * x\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                    else:\n",
    "                        print(f\"No magnitude adjustment could be made for {event}\")\n",
    "\n",
    "            ### Define Rupture Context\n",
    "            \n",
    "            rctx = RuptureContext()\n",
    "            rctx.mag = magnitude\n",
    "        \n",
    "            #try to find focal mechanism to add rake and dip\n",
    "            try:\n",
    "                mech = cat.focal_mechanisms[0]\n",
    "                nop = mech.nodal_planes['nodal_plane_1']\n",
    "                rctx.rake = nop['rake']\n",
    "                rctx.dip = nop['dip']\n",
    "                print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "            except:\n",
    "                rctx.rake = 0\n",
    "                rctx.dip = 45\n",
    "                print(f'No focal mechanism available for {magnitude} {event}')\n",
    "                \n",
    "            # assume square ruptures with identical stress drop for every event\n",
    "            moment = 10 ** (1.5*(magnitude+10.7))\n",
    "            moment = moment * 10**-7\n",
    "            stressdrop = 3*10**6 #Pa\n",
    "            L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "            L = L/1000 #km \n",
    "            rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "            rctx.width = L #length of square\n",
    "            rctx.hypo_depth = depth\n",
    "            \n",
    "            ### Define Distance Context (assume fault-perpendicular sites)\n",
    "            \n",
    "            dctx = DistancesContext()\n",
    "            rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "            rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "            npts = rrups.size # number of distance points\n",
    "            dctx.rrup = rrups # Closest distance to rupture surface\n",
    "            dctx.rjb = rjbs # Distance to rupture’s surface projection\n",
    "            dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "            dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                     #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "            \n",
    "            distancestostore = df['RuptureDistance'].tolist()\n",
    "            totaldistances.extend(distancestostore)\n",
    "            \n",
    "            ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "            \n",
    "            #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "            # Prepare results\n",
    "            vs30_values = []\n",
    "            measured_flags = []\n",
    "        \n",
    "            #Find Vs30 measurements from USGS raster\n",
    "            # Iterate over stations\n",
    "            for _, station in df.iterrows():\n",
    "                lat = station['StationLatitude']\n",
    "                lon = station['StationLongitude']\n",
    "                # Query tree for neighbors within the threshold\n",
    "                distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "                index = indices[0]\n",
    "                if index != tree.n:  # Found a valid neighbor\n",
    "                    vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                    measured = True\n",
    "                else:\n",
    "                    # Fallback to raster\n",
    "                    row, col = raster.index(lon, lat)\n",
    "                    if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                        vs30 = raster_array[row, col]\n",
    "                    else:\n",
    "                        vs30 = np.nan\n",
    "                    measured = False\n",
    "                vs30_values.append(vs30)\n",
    "                measured_flags.append(measured)\n",
    "            \n",
    "            # Add columns to stations DataFrame\n",
    "            df['Vs30'] = vs30_values\n",
    "            df['measured'] = measured_flags\n",
    "            sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                            'vs30': vs30_values, # Vs30 value in m/s\n",
    "                            'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                            'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                            'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                            }\n",
    "            \"\"\"'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\"\"\"\n",
    "            sitecollection = pd.DataFrame(sitecol_dict)\n",
    "            sctx = SitesContext(sitecol=sitecollection)   \n",
    "            \n",
    "            ### Calculate Ground Motions\n",
    "            \n",
    "            # define metrics\n",
    "            #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "            IMT = imt.PGV()\n",
    "            #IMT = imt.SA(period=0.1)\n",
    "            uncertaintytype = const.StdDev.TOTAL\n",
    "            \n",
    "            # instantiate GMMs\n",
    "            ASK14 = AbrahamsonEtAl2014()\n",
    "            BSSA14 = BooreEtAl2014()\n",
    "            CB14 = CampbellBozorgnia2014()\n",
    "            CY14 = ChiouYoungs2014()\n",
    "            \n",
    "            # calculate ln ground motions for each\n",
    "            ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            \n",
    "            # average across all four for active crustal values\n",
    "            ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "            ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "        \n",
    "            ### Find Observed Data\n",
    "            \n",
    "            # gather observed ground motion metrics\n",
    "            OBSdrup = df['RuptureDistance']\n",
    "            OBSpga = df['PGA']*0.01 #%g\n",
    "            OBSpgv = df['PGV'] #cm/s\n",
    "            OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "        \n",
    "            ''' identify which metric is being worked with '''\n",
    "            ObservedMetric = OBSpgv\n",
    "            \n",
    "            # calculate residuals with estimated data\n",
    "            residuals = []\n",
    "            for i in range(len(ln_median_ac14)):\n",
    "                residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "                residuals.append(residual)\n",
    "            residuals = np.array(residuals)\n",
    "            df['Residuals']= residuals\n",
    "            mean_residual = np.mean(residuals)\n",
    "            totalresiduals.append(mean_residual)\n",
    "            mean_residual = round(mean_residual, 4)\n",
    "            totalstationresiduals.extend(residuals)\n",
    "        \n",
    "            # Find SNR values\n",
    "            snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "            # Prepare list to store the minimum SNRs for each StationID\n",
    "            min_snrs = []\n",
    "            # Iterate through each StationID\n",
    "            for station_id in df['StationID']:\n",
    "                station_id = str(station_id)  # Make sure it's a string\n",
    "                # Update count for this StationID\n",
    "                station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "                # Find rows in snr_df where TraceID starts with the StationID\n",
    "                matches = snr_df[\n",
    "                    snr_df['TraceID'].astype(str).str.startswith(str(station_id)) &\n",
    "                    ~snr_df['TraceID'].astype(str).str.endswith('Z')\n",
    "                ]\n",
    "            \n",
    "                if not matches.empty:\n",
    "                    min_snr = matches['SNR(1)'].min()\n",
    "                    # Add the result to the original DataFrame\n",
    "                    min_snrs.append(min_snr)\n",
    "                    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "                \n",
    "                else:\n",
    "                    min_snr = float('0') # Or some default, like None or 0\n",
    "                    print('Did not find stationID in SNR csv')\n",
    "                \n",
    "            df['MinSNR'] = min_snrs\n",
    "                \n",
    "            ### Plot Model Results\n",
    "            \n",
    "            # figure setup\n",
    "            fig, axi = plt.subplots(figsize=(6,4))\n",
    "            axi.set_facecolor(\"gainsboro\")\n",
    "            # Create a colormap\n",
    "            cmap = plt.colormaps['Reds']\n",
    "            # Normalize the values to the range 0-1 for the colormap\n",
    "            #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "            #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "            norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "            \n",
    "            # plot GMPE\n",
    "            ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "            ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "            axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "            plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "            #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "            axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "            #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "            #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "            \n",
    "            # formatting\n",
    "            plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "            axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "            if ObservedMetric.equals(OBSpgv):\n",
    "                axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "            if ObservedMetric.equals(OBSpga):\n",
    "                axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "            if ObservedMetric.equals(OBSsa):\n",
    "                axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "            axi.grid(lw=0.3)\n",
    "            axi.legend(loc=\"upper right\",fontsize=11)\n",
    "            axi.set_xlim(9,320)\n",
    "            plt.xscale('log')\n",
    "            plt.yscale('log')\n",
    "            plt.colorbar(label='SNR')\n",
    "        \n",
    "            pdf_pages.savefig(fig)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            print('Outside Magnitude Range')\n",
    "    else:\n",
    "        print('Outside Date Range')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"/OpenquakeFigures/ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "pdf_pages.close()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].scatter(totalmagnitudes, totalresiduals, s=2)\n",
    "axes[0].set_title('Magnitude vs Event Residual')\n",
    "axes[0].set_xlabel('Magnitude')\n",
    "axes[0].set_ylabel('Mean Residual')\n",
    "\n",
    "axes[1].scatter(totaldistances, totalstationresiduals, s=1)\n",
    "axes[1].set_title('Distance vs Station Residual')\n",
    "axes[1].set_xlabel('Rrup (km)')\n",
    "axes[1].set_ylabel('Station Residual Difference')\n",
    "plt.savefig(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/maganddistwithresiduals.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load event IDs from CSV\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "event_ids = eventdata['eventID']\n",
    "\n",
    "# Initialize list to store results\n",
    "row_counts = []\n",
    "\n",
    "# Loop through each event\n",
    "for event in event_ids:\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            row_count = len(df)  # includes data rows only\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "            row_count = -1  # flag for read error\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        row_count = 0  # file missing\n",
    "\n",
    "    # Store result\n",
    "    row_counts.append({'eventID': event, 'NumRows': row_count})\n",
    "\n",
    "# Save results to CSV\n",
    "counts_df = pd.DataFrame(row_counts)\n",
    "counts_df = counts_df.sort_values(by='NumRows', ascending=True)\n",
    "\n",
    "# Print summary\n",
    "print(counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNR Ratio Assembly\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"ParkerButteTestCSV.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    # Find SNR values\n",
    "            snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "            # Prepare list to store the minimum SNRs for each StationID\n",
    "            min_snrs = []\n",
    "            # Iterate through each StationID\n",
    "            for station_id in df['StationID']:\n",
    "                station_id = str(station_id)  # Make sure it's a string\n",
    "                # Update count for this StationID\n",
    "                station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "                # Find rows in snr_df where TraceID starts with the StationID\n",
    "                matches = snr_df[\n",
    "                    snr_df['TraceID'].astype(str).str.startswith(str(station_id)) &\n",
    "                    ~snr_df['TraceID'].astype(str).str.endswith('Z')\n",
    "                ]\n",
    "            \n",
    "                if not matches.empty:\n",
    "                    min_snr = matches['SNR(1)'].min()\n",
    "                    # Add the result to the original DataFrame\n",
    "                    min_snrs.append(min_snr)\n",
    "                    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "                \n",
    "                else:\n",
    "                    min_snr = float('0') # Or some default, like None or 0\n",
    "                    print('Did not find stationID in SNR csv')\n",
    "                \n",
    "            df['MinSNR'] = min_snrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # NEW for saving all plots\n",
    "\n",
    "# Load event list\n",
    "events_df = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "event_ids = events_df['eventID'].dropna().astype(str).tolist()\n",
    "\n",
    "# Thresholds and bins\n",
    "thresholds = [1, 2, 3, 5, 8, 10, 20]\n",
    "magnitude_bins = [(2.5, 3), (3, 4), (4, 5), (5, 6)]\n",
    "\n",
    "# Storage\n",
    "all_data = []  # Store all SNR rows with their magnitude and EarthquakeId\n",
    "\n",
    "# Stats\n",
    "missing_files = 0\n",
    "events_processed = 0\n",
    "\n",
    "for event in event_ids:\n",
    "    snr_path = f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\"\n",
    "\n",
    "    if not os.path.exists(snr_path):\n",
    "        print(f\"Missing file: {event}\")\n",
    "        missing_files += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(snr_path)\n",
    "        if 'EarthquakeMagnitude' not in df.columns:\n",
    "            print(f\"Missing magnitude column in {event}\")\n",
    "            continue\n",
    "\n",
    "        snr_columns = df.columns[-16:]\n",
    "        mag = df['EarthquakeMagnitude'].values[0]\n",
    "        snr_df = df[snr_columns].copy()\n",
    "        snr_df['Magnitude'] = mag\n",
    "        snr_df['EarthquakeId'] = df['EarthquakeId']\n",
    "\n",
    "        all_data.append(snr_df)\n",
    "        events_processed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {event}: {e}\")\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Frequencies from SNR columns\n",
    "frequency_labels = combined_df.columns[:-2].tolist()  # exclude 'Magnitude' and 'EarthquakeId'\n",
    "\n",
    "# Function to calculate failure matrix\n",
    "def calculate_failure_matrix(data, thresholds, freq_labels):\n",
    "    failure_counts = pd.DataFrame(0, index=freq_labels, columns=thresholds)\n",
    "    total = data.shape[0]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        failures = (data[freq_labels] < threshold).sum()\n",
    "        failure_counts[threshold] = failures\n",
    "\n",
    "    return failure_counts, total\n",
    "\n",
    "# Function to plot heatmap with annotations\n",
    "def plot_heatmap(failure_counts, total, n_events, title_prefix):\n",
    "    heatmap_data = failure_counts.astype(int).T\n",
    "    annotations = heatmap_data.copy().astype(str)\n",
    "\n",
    "    for t in heatmap_data.index:\n",
    "        for f in heatmap_data.columns:\n",
    "            count = heatmap_data.at[t, f]\n",
    "            percent = (count / total) * 100 if total > 0 else 0\n",
    "            annotations.at[t, f] = f\"{count} ({percent:.1f}%)\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        heatmap_data.T,\n",
    "        annot=annotations.T,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Reds\",\n",
    "        cbar_kws={'label': 'Count of Failing Recordings'}\n",
    "    )\n",
    "    plt.title(f\"{title_prefix} [{n_events} Events, {total} Recordings]\")\n",
    "    plt.xlabel(\"SNR Threshold\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Save all plots into one PDF\n",
    "with PdfPages(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/SNR_heatmaps.pdf\") as pdf:\n",
    "    # All data\n",
    "    failures_all, total_all = calculate_failure_matrix(combined_df, thresholds, frequency_labels)\n",
    "    n_events_all = combined_df['EarthquakeId'].nunique()\n",
    "    plot_heatmap(failures_all, total_all, n_events_all, \"SNR Failures (All Magnitudes)\")\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "    # Each magnitude bin\n",
    "    for lower, upper in magnitude_bins:\n",
    "        bin_df = combined_df[(combined_df['Magnitude'] >= lower) & (combined_df['Magnitude'] < upper)]\n",
    "        if bin_df.empty:\n",
    "            print(f\"No data for magnitude range {lower}–{upper}\")\n",
    "            continue\n",
    "\n",
    "        failures_bin, total_bin = calculate_failure_matrix(bin_df, thresholds, frequency_labels)\n",
    "        n_events_bin = bin_df['EarthquakeId'].nunique()\n",
    "        plot_heatmap(failures_bin, total_bin, n_events_bin, f\"SNR Failures (Magnitude {lower}–{upper})\")\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "# Summary output\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "print(f\"Total recordings analyzed: {combined_df.shape[0]}\")\n",
    "print(f\"Unique events analyzed: {combined_df['EarthquakeId'].nunique()}\")\n",
    "print(f\"Events processed: {events_processed}\")\n",
    "print(f\"Missing SNR files: {missing_files}\")\n",
    "print(f\"Total events listed: {len(event_ids)}\")\n",
    "print(\"Saved heatmaps to SNR_heatmaps.pdf ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
