{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Python Notebook to Test Openquake and Pygmm\n",
    "# Daniel Trugman, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from obspy.clients.fdsn import Client\n",
    "import rasterio\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#sns.set_style(\"darkgrid\")\n",
    "\n",
    "# openquake - generic\n",
    "# - https://docs.openquake.org/oq-engine/3.5/baselib.html#installation\n",
    "#  - worked ok as !pip install openquake.engine in python 3.10 environment...\n",
    "#    - but needed to add fiona afterward with mamba\n",
    "# - for documentation see this:\n",
    "# - https://docs.openquake.org/oq-engine/3.5/openquake.hazardlib.html\n",
    "from openquake.hazardlib.contexts import RuptureContext\n",
    "from openquake.hazardlib.contexts import DistancesContext\n",
    "from openquake.hazardlib.contexts import SitesContext\n",
    "from openquake.hazardlib import imt, const\n",
    "\n",
    "# active crustal models\n",
    "from openquake.hazardlib.gsim.abrahamson_2014 import AbrahamsonEtAl2014\n",
    "from openquake.hazardlib.gsim.boore_2014 import BooreEtAl2014\n",
    "from openquake.hazardlib.gsim.campbell_bozorgnia_2014 import CampbellBozorgnia2014\n",
    "from openquake.hazardlib.gsim.chiou_youngs_2014 import ChiouYoungs2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "\n",
    "## Define Rupture Context\n",
    "rctx = RuptureContext()\n",
    "rctx.mag = 5.7\n",
    "rctx.rake = -19\n",
    "rctx.dip = 80\n",
    "rctx.ztor = 0.1 #guessing\n",
    "rctx.width = 10 # guessing\n",
    "rctx.hypo_depth = 9.3 \n",
    "\n",
    "## Define Distance Context (assume fault-perpendicular sites)\n",
    "dctx = DistancesContext()\n",
    "rjbs = np.linspace(0.0, 320.0, 321) # rjb values in km\n",
    "rrups = np.sqrt(rjbs**2 + rctx.ztor**2) # calculated rupture distance for a vertical fault\n",
    "npts = rrups.size # number of distance points\n",
    "dctx.rrup = rrups # Closest distance to rupture surface\n",
    "dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                         #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "\n",
    "\n",
    "## Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "#    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                }\n",
    "sitecollection = pd.DataFrame(sitecol_dict)\n",
    "sctx = SitesContext(sitecol=sitecollection)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Calculate Ground Motions\n",
    "\n",
    "# define metrics\n",
    "#IMT = imt.PGA()\n",
    "IMT = imt.PGV()\n",
    "#IMT = imt.SA(period=0.1)\n",
    "uncertaintytype = const.StdDev.TOTAL\n",
    "\n",
    "# instantiate GMMs\n",
    "ASK14 = AbrahamsonEtAl2014()\n",
    "BSSA14 = BooreEtAl2014()\n",
    "CB14 = CampbellBozorgnia2014()\n",
    "CY14 = ChiouYoungs2014()\n",
    "\n",
    "# calculate ln ground motions for each\n",
    "ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "\n",
    "# average across all four for active crustal values\n",
    "ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Find Observed Data\n",
    "\n",
    "df = pd.read_csv('nn00888580_default_metrics_rotd(percentile=50.0).csv')\n",
    "OBSdrup = df['RuptureDistance']\n",
    "OBSpga = df['PGA']\n",
    "OBSpgv = df['PGV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Plot Model Results\n",
    "\n",
    "# figure setup\n",
    "fig, axi = plt.subplots(figsize=(6,4))\n",
    "axi.set_facecolor(\"gainsboro\")\n",
    "\n",
    "# plot GMPE\n",
    "ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "plt.scatter(OBSdrup, OBSpgv, s=5, label=\"Parker Butte M5.7 Observed Motions\", zorder=0) #OBSpgv or OBSpga\n",
    "axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "#axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "#axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "\n",
    "# formatting\n",
    "axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "axi.grid(lw=0.3)\n",
    "axi.legend(loc=\"upper right\",fontsize=11)\n",
    "axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show results\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- basin term and vs30 defaults\n",
    "- optimize rupture context\n",
    "- optimize distance metrics?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 45\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30':760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpgv\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(totalresiduals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGA)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 90\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    #IMT = imt.PGV()\n",
    "    IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSsa\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGA)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 90\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "        #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "    IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    #IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpga\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14 #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, s=5, label=f\"{event} Observed Motions\", zorder=0)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (PLOTTING)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth'] \n",
    "\n",
    "    ## Define Rupture Context\n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude #PB 5.7\n",
    "    rctx.rake = 0 #PB -19\n",
    "    rctx.dip = 90 #PB 80\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km\n",
    "    \n",
    "    rctx.ztor = depth-L/2 #guessing\n",
    "    rctx.width =L # guessing\n",
    "    rctx.hypo_depth = depth #PB 9.3 \n",
    "    \n",
    "    ## Define Distance Context (assume fault-perpendicular sites)\n",
    "    dctx = DistancesContext()\n",
    "    rjbs = np.linspace(0.0, 320.0, 321) # rjb values in km\n",
    "    rrups = np.sqrt(rjbs**2 + rctx.ztor**2) # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    \n",
    "    ## Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA()\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1\n",
    "    \n",
    "    ### Find Observed Data\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"     #FIX THIS WITH EVENT DIRECTORY STUFF FROM ANALYZER\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA'] #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    \n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, OBSpgv, s=5, label=f\"M{magnitude} {event} Observed Motions\", zorder=0) #OBSpgv or OBSpga\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=2)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    #axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show results\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30':760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': np.zeros(npts, dtype=bool), # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "# Prepare results\n",
    "vs30_values = []\n",
    "measured_flags = []\n",
    "\n",
    "# Iterate over stations\n",
    "for _, station in df.iterrows():\n",
    "    lat = station['StationLatitude']\n",
    "    lon = station['StationLongitude']\n",
    "    \n",
    "    # Query tree for neighbors within the threshold\n",
    "    distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "    index = indices[0]\n",
    "    \n",
    "    if index != tree.n:  # Found a valid neighbor\n",
    "        vs30 = vs30_df.iloc[index]['Vs30']\n",
    "        measured = True\n",
    "    else:\n",
    "        # Fallback to raster\n",
    "        row, col = raster.index(lon, lat)\n",
    "        if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "            vs30 = raster_array[row, col]\n",
    "        else:\n",
    "            vs30 = np.nan\n",
    "        measured = False\n",
    "    \n",
    "    vs30_values.append(vs30)\n",
    "    measured_flags.append(measured)\n",
    "\n",
    "# Add columns to stations DataFrame\n",
    "df['Vs30'] = vs30_values\n",
    "df['measured'] = measured_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV, Vs30 colorbar, can use time subset)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    #Comment this section out if you just want to use whole dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        '''WHAT DOES SQUARE RUPTURE BIAS TOWARD IN TERMS OF GMM COMPARED TO RANGE OF RUPTURE STYLES? GUESSING IT LEADS TO SMALLER PREDICTED MOTION\n",
    "            #THAN REALISTIC RUPTURE SCENARIOS DUE TO GREATER DEPTH AND LESS WIDTH THAN RECTANGULAR PLANES\n",
    "        IT WOULD BE INTERESTING TO TRY THIS WITH CIRCLES/ELLIPSES TOO AND COMPARE THE RESULTING RESIDUALS'''\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            \n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            \n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            \n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1    \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['seismic']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=vs30_values, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='Vs30')\n",
    "\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print('Out of Date Range')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "pdf_pages.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#All together (RESIDUAL CALCULATIONS PGV, SNR colorbar, can use time or mag subset)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3\n",
    "    maxmag = 5.7\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].max())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='Vs30')\n",
    "\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print('Outside magnitude')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "pdf_pages.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SNR Analysis\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "        \n",
    "    event = row['eventID']\n",
    "    magnitude = row['EarthquakeMagnitude']\n",
    "    depth = row['EarthquakeDepth']\n",
    "    \n",
    "    # download more rupture data for the event\n",
    "    cat = client.get_events(eventid=f\"{event}\")\n",
    "    cat = cat[0]\n",
    "    \n",
    "    #find event specific data from gmprocess (Reno1)\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    ### Define Rupture Context\n",
    "    \n",
    "    rctx = RuptureContext()\n",
    "    rctx.mag = magnitude\n",
    "\n",
    "    #try to find focal mechanism to add rake and dip\n",
    "    try:\n",
    "        mech = cat.focal_mechanisms[0]\n",
    "        nop = mech.nodal_planes['nodal_plane_1']\n",
    "        rctx.rake = nop['rake']\n",
    "        rctx.dip = nop['dip']\n",
    "        print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "    except:\n",
    "        rctx.rake = 0\n",
    "        rctx.dip = 45\n",
    "        \n",
    "    # assume square ruptures with identical stress drop for every event\n",
    "    moment = 10 ** (1.5*(magnitude+10.7))\n",
    "    stressdrop = 3*10**6 #Pa\n",
    "    L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "    L = L/1000 #km \n",
    "    rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "    rctx.width =L #length of square\n",
    "    rctx.hypo_depth = depth\n",
    "    \n",
    "    ### Define Distance Context (assume fault-perpendicular sites)\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "    rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "    npts = rrups.size # number of distance points\n",
    "    dctx.rrup = rrups # Closest distance to rupture surface\n",
    "    dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "    dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "    dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                             #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "    \n",
    "    ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "    \n",
    "    #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "    # Prepare results\n",
    "    vs30_values = []\n",
    "    measured_flags = []\n",
    "\n",
    "    #Find Vs30 measurements from USGS raster\n",
    "    # Iterate over stations\n",
    "    for _, station in df.iterrows():\n",
    "        lat = station['StationLatitude']\n",
    "        lon = station['StationLongitude']\n",
    "        # Query tree for neighbors within the threshold\n",
    "        distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "        index = indices[0]\n",
    "        if index != tree.n:  # Found a valid neighbor\n",
    "            vs30 = vs30_df.iloc[index]['Vs30']\n",
    "            measured = True\n",
    "        else:\n",
    "            # Fallback to raster\n",
    "            row, col = raster.index(lon, lat)\n",
    "            if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                vs30 = raster_array[row, col]\n",
    "            else:\n",
    "                vs30 = np.nan\n",
    "            measured = False\n",
    "        vs30_values.append(vs30)\n",
    "        measured_flags.append(measured)\n",
    "    \n",
    "    # Add columns to stations DataFrame\n",
    "    df['Vs30'] = vs30_values\n",
    "    df['measured'] = measured_flags\n",
    "    sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                    'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                    'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                    'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                    'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                    }\n",
    "    sitecollection = pd.DataFrame(sitecol_dict)\n",
    "    sctx = SitesContext(sitecol=sitecollection)   \n",
    "    \n",
    "    ### Calculate Ground Motions\n",
    "    \n",
    "    # define metrics\n",
    "    #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "    IMT = imt.PGV()\n",
    "    #IMT = imt.SA(period=0.1)\n",
    "    uncertaintytype = const.StdDev.TOTAL\n",
    "    \n",
    "    # instantiate GMMs\n",
    "    ASK14 = AbrahamsonEtAl2014()\n",
    "    BSSA14 = BooreEtAl2014()\n",
    "    CB14 = CampbellBozorgnia2014()\n",
    "    CY14 = ChiouYoungs2014()\n",
    "    \n",
    "    # calculate ln ground motions for each\n",
    "    ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "    \n",
    "    # average across all four for active crustal values\n",
    "    ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "    ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "\n",
    "    ### Find Observed Data\n",
    "    \n",
    "    # gather observed ground motion metrics\n",
    "    OBSdrup = df['RuptureDistance']\n",
    "    OBSpga = df['PGA']*0.01 #%g\n",
    "    OBSpgv = df['PGV'] #cm/s\n",
    "    OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "\n",
    "    ''' identify which metric is being worked with '''\n",
    "    ObservedMetric = OBSpgv\n",
    "    \n",
    "    # calculate residuals with estimated data\n",
    "    residuals = []\n",
    "    for i in range(len(ln_median_ac14)):\n",
    "        residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "        residuals.append(residual)\n",
    "    residuals = np.array(residuals)\n",
    "    df['Residuals']= residuals\n",
    "    mean_residual = np.mean(residuals)\n",
    "    totalresiduals.append(mean_residual)\n",
    "    mean_residual = round(mean_residual, 4)\n",
    "\n",
    "    # (optional) find SNR values\n",
    "    snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "    # Prepare list to store the minimum SNRs for each StationID\n",
    "    min_snrs = []\n",
    "    # Iterate through each StationID\n",
    "    for station_id in df['StationID']:\n",
    "        station_id = str(station_id)  # Make sure it's a string\n",
    "        # Update count for this StationID\n",
    "        station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "        # Find rows in snr_df where TraceID starts with the StationID\n",
    "        matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "    \n",
    "        if not matches.empty:\n",
    "            min_snr = matches['SNR(1)'].max()\n",
    "        else:\n",
    "            min_snr = float('0') # Or some default, like None or 0\n",
    "            print('Did not find stationID in SNR csv')\n",
    "        min_snrs.append(min_snr)\n",
    "    \n",
    "    # Add the result to the original DataFrame\n",
    "    df['MinSNR'] = min_snrs\n",
    "    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "    \n",
    "    ### Plot Model Results\n",
    "    \n",
    "    # figure setup\n",
    "    fig, axi = plt.subplots(figsize=(6,4))\n",
    "    axi.set_facecolor(\"gainsboro\")\n",
    "     # Create a colormap\n",
    "    cmap = plt.colormaps['Reds']\n",
    "    # Normalize the values to the range 0-1 for the colormap\n",
    "    #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "    #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "    norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "    \n",
    "    # plot GMPE\n",
    "    ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "    ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "    axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "    plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "    #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "    axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "    #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "    #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "    \n",
    "    # formatting\n",
    "    plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "    axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpgv):\n",
    "        axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "    if ObservedMetric.equals(OBSpga):\n",
    "        axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "    if ObservedMetric.equals(OBSsa):\n",
    "        axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "    axi.grid(lw=0.3)\n",
    "    axi.legend(loc=\"upper right\",fontsize=11)\n",
    "    axi.set_xlim(9,320)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.colorbar(label='SNR')\n",
    "\n",
    "    #pdf_pages.savefig(fig)\n",
    "    #plt.close(fig)\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#SNR Analysis (Adding magnitude)\n",
    " \n",
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3\n",
    "    maxmag = 5.7\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        \n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 3*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            station_id = str(station_id)  # Make sure it's a string\n",
    "            # Update count for this StationID\n",
    "            station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "        norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='SNR')\n",
    "    \n",
    "        #pdf_pages.savefig(fig)\n",
    "        #plt.close(fig)\n",
    "    else:\n",
    "        print('Out of magnitude range')\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "totalmagnitudes = []\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "#pdf_pages = PdfPages('PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    '''#Comment this section out if you just want to use whole timeline\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2008-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2009-01-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:'''\n",
    "\n",
    "    mag = row['EarthquakeMagnitude']\n",
    "    minmag = 3.5\n",
    "    maxmag = 4\n",
    "    # Check if mag is between min and max\n",
    "    if minmag <= mag <= maxmag:\n",
    "        \n",
    "        event = row['eventID']\n",
    "        magnitude = row['EarthquakeMagnitude']\n",
    "        depth = row['EarthquakeDepth']\n",
    "        totalmagnitudes.append(magnitude)\n",
    "        # download more rupture data for the event\n",
    "        cat = client.get_events(eventid=f\"{event}\")\n",
    "        cat = cat[0]\n",
    "        \n",
    "        #find event specific data from gmprocess (Reno1)\n",
    "        base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "        filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "        file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "        ### Define Rupture Context\n",
    "        \n",
    "        rctx = RuptureContext()\n",
    "        rctx.mag = magnitude\n",
    "    \n",
    "        #try to find focal mechanism to add rake and dip\n",
    "        try:\n",
    "            mech = cat.focal_mechanisms[0]\n",
    "            nop = mech.nodal_planes['nodal_plane_1']\n",
    "            rctx.rake = nop['rake']\n",
    "            rctx.dip = nop['dip']\n",
    "            print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "        except:\n",
    "            rctx.rake = 0\n",
    "            rctx.dip = 45\n",
    "            \n",
    "        # assume square ruptures with identical stress drop for every event\n",
    "        moment = 10 ** (1.5*(magnitude+10.7))\n",
    "        stressdrop = 10*10**6 #Pa\n",
    "        L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "        L = L/1000 #km \n",
    "        rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "        rctx.width =L #length of square\n",
    "        rctx.hypo_depth = depth\n",
    "        \n",
    "        ### Define Distance Context (assume fault-perpendicular sites)\n",
    "        \n",
    "        dctx = DistancesContext()\n",
    "        rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "        rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "        npts = rrups.size # number of distance points\n",
    "        dctx.rrup = rrups # Closest distance to rupture surface\n",
    "        dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "        dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "        dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                 #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "        \n",
    "        ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "        \n",
    "        #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "        # Prepare results\n",
    "        vs30_values = []\n",
    "        measured_flags = []\n",
    "    \n",
    "        #Find Vs30 measurements from USGS raster\n",
    "        # Iterate over stations\n",
    "        for _, station in df.iterrows():\n",
    "            lat = station['StationLatitude']\n",
    "            lon = station['StationLongitude']\n",
    "            # Query tree for neighbors within the threshold\n",
    "            distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "            index = indices[0]\n",
    "            if index != tree.n:  # Found a valid neighbor\n",
    "                vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                measured = True\n",
    "            else:\n",
    "                # Fallback to raster\n",
    "                row, col = raster.index(lon, lat)\n",
    "                if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                    vs30 = raster_array[row, col]\n",
    "                else:\n",
    "                    vs30 = np.nan\n",
    "                measured = False\n",
    "            vs30_values.append(vs30)\n",
    "            measured_flags.append(measured)\n",
    "        \n",
    "        # Add columns to stations DataFrame\n",
    "        df['Vs30'] = vs30_values\n",
    "        df['measured'] = measured_flags\n",
    "        sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                        'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\n",
    "                        'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                        'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                        'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                        }\n",
    "        sitecollection = pd.DataFrame(sitecol_dict)\n",
    "        sctx = SitesContext(sitecol=sitecollection)   \n",
    "        \n",
    "        ### Calculate Ground Motions\n",
    "        \n",
    "        # define metrics\n",
    "        #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "        IMT = imt.PGV()\n",
    "        #IMT = imt.SA(period=0.1)\n",
    "        uncertaintytype = const.StdDev.TOTAL\n",
    "        \n",
    "        # instantiate GMMs\n",
    "        ASK14 = AbrahamsonEtAl2014()\n",
    "        BSSA14 = BooreEtAl2014()\n",
    "        CB14 = CampbellBozorgnia2014()\n",
    "        CY14 = ChiouYoungs2014()\n",
    "        \n",
    "        # calculate ln ground motions for each\n",
    "        ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "        \n",
    "        # average across all four for active crustal values\n",
    "        ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "        ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "    \n",
    "        ### Find Observed Data\n",
    "        \n",
    "        # gather observed ground motion metrics\n",
    "        OBSdrup = df['RuptureDistance']\n",
    "        OBSpga = df['PGA']*0.01 #%g\n",
    "        OBSpgv = df['PGV'] #cm/s\n",
    "        OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "    \n",
    "        ''' identify which metric is being worked with '''\n",
    "        ObservedMetric = OBSpgv\n",
    "        \n",
    "        # calculate residuals with estimated data\n",
    "        residuals = []\n",
    "        for i in range(len(ln_median_ac14)):\n",
    "            residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "            residuals.append(residual)\n",
    "        residuals = np.array(residuals)\n",
    "        df['Residuals']= residuals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        totalresiduals.append(mean_residual)\n",
    "        mean_residual = round(mean_residual, 4)\n",
    "    \n",
    "        # (optional) find SNR values\n",
    "        snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "        # Prepare list to store the minimum SNRs for each StationID\n",
    "        min_snrs = []\n",
    "        # Iterate through each StationID\n",
    "        for station_id in df['StationID']:\n",
    "            station_id = str(station_id)  # Make sure it's a string\n",
    "            # Update count for this StationID\n",
    "            station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "            # Find rows in snr_df where TraceID starts with the StationID\n",
    "            matches = snr_df[snr_df['TraceID'].astype(str).str.startswith(str(station_id))]\n",
    "        \n",
    "            if not matches.empty:\n",
    "                min_snr = matches['SNR(1)'].min()\n",
    "            else:\n",
    "                min_snr = float('0') # Or some default, like None or 0\n",
    "                print('Did not find stationID in SNR csv')\n",
    "            min_snrs.append(min_snr)\n",
    "        \n",
    "        # Add the result to the original DataFrame\n",
    "        df['MinSNR'] = min_snrs\n",
    "        station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "        \n",
    "        ### Plot Model Results\n",
    "        \n",
    "        # figure setup\n",
    "        fig, axi = plt.subplots(figsize=(6,4))\n",
    "        axi.set_facecolor(\"gainsboro\")\n",
    "         # Create a colormap\n",
    "        cmap = plt.colormaps['Reds']\n",
    "        # Normalize the values to the range 0-1 for the colormap\n",
    "        #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "        #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "        norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "        \n",
    "        # plot GMPE\n",
    "        ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "        ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "        axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "        plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "        #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "        axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "        #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "        #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "        \n",
    "        # formatting\n",
    "        plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "        axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpgv):\n",
    "            axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "        if ObservedMetric.equals(OBSpga):\n",
    "            axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "        if ObservedMetric.equals(OBSsa):\n",
    "            axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "        axi.grid(lw=0.3)\n",
    "        axi.legend(loc=\"upper right\",fontsize=11)\n",
    "        axi.set_xlim(9,320)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.colorbar(label='SNR')\n",
    "    \n",
    "        #pdf_pages.savefig(fig)\n",
    "        #plt.close(fig)\n",
    "    else:\n",
    "        print('Out of magnitude range')\n",
    "\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "#plt.savefig(\"ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "#pdf_pages.close()\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup for Calculations\n",
    "\n",
    "#initialize list to store residuals for each event\n",
    "totalresiduals = []\n",
    "totalmagnitudes = []\n",
    "totaldistances = []\n",
    "totalstationresiduals = []\n",
    "allSNRrecordings = pd.DataFrame()\n",
    "#setup client for rupture geometry data\n",
    "client = Client(\"USGS\")\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"ParkerButteTestCSV.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "#Prepare files for Vs30 analysis\n",
    "# Load Vs30 CSV\n",
    "vs30_df = pd.read_csv(f'/Users/carterdills/Documents/QGIS/RenoVS30.csv')\n",
    "# Threshold distance in degrees\n",
    "threshold_deg = 0.0001\n",
    "# Build KDTree from vs30 points\n",
    "tree_coords = vs30_df[['Latitude', 'Longitude']].to_numpy()\n",
    "tree = cKDTree(tree_coords)\n",
    "# Load raster\n",
    "raster = rasterio.open(f'/Users/carterdills/Documents/vs30/vs30_mosaic.tif')\n",
    "raster_array = raster.read(1)\n",
    "transform = raster.transform\n",
    "\n",
    "pdf_pages = PdfPages('/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/PGVoutputs.pdf')\n",
    "station_snr_records = []\n",
    "station_counts = {}       # Will hold {StationID: count}\n",
    "\n",
    "#loop through events to plot gm metrics for each\n",
    "for _, row in eventdata.iterrows():\n",
    "\n",
    "    #Comment this section out if you just want to use whole event dataset\n",
    "    dt = datetime.fromisoformat(row['EarthquakeTime'])\n",
    "    start = datetime.fromisoformat('2000-01-01T00:00:00.000000Z')\n",
    "    end = datetime.fromisoformat('2025-06-01T00:00:00.000000Z')\n",
    "    # Check if dt is between start and end\n",
    "    if start <= dt <= end:\n",
    "\n",
    "        mag = row['EarthquakeMagnitude']\n",
    "        minmag = 2.5\n",
    "        maxmag = 5.7\n",
    "        # Check if mag is between min and max\n",
    "        if minmag <= mag <= maxmag:\n",
    "            \n",
    "            event = row['eventID']\n",
    "            magnitude = row['EarthquakeMagnitude']\n",
    "            depth = row['EarthquakeDepth']\n",
    "            totalmagnitudes.append(magnitude)\n",
    "            # download more rupture data for the event\n",
    "            cat = client.get_events(eventid=f\"{event}\")\n",
    "            cat = cat[0]\n",
    "            \n",
    "            #find event specific data from gmprocess (Reno1)\n",
    "            base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "            filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "            file_path = os.path.join(base_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = df.sort_values(by='RuptureDistance', ascending =True)\n",
    "                df = df.reset_index(drop=True)\n",
    "\n",
    "            ### Determine Magnitude Type\n",
    "            try:\n",
    "                value = df.iat[1, 6]\n",
    "                if pd.notna(value):\n",
    "                    magnitudetype = value.strip().lower()\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            except (IndexError, ValueError, AttributeError):\n",
    "                print(f\"Didn't find magnitudetype for {event}\")\n",
    "                magnitudetype = \"mw\"\n",
    "\n",
    "            if magnitudetype != \"mw\":\n",
    "                try:\n",
    "                    nil_df = pd.read_csv(f'/Users/carterdills/Documents/nsl_webmt.csv')\n",
    "                    event_suffix = str(event)[-6:]\n",
    "                    if event_suffix in nil_df.iloc[:, 0].astype(str).values:\n",
    "                        print(f\"Replaced {magnitudetype} with mw for {event}\")\n",
    "                        magnitudetype = \"mw\"\n",
    "                    else:\n",
    "                        print(f\"Converted {magnitudetype} for {event} mathematically\")\n",
    "\n",
    "                        x = magnitude  # store current value\n",
    "                        if magnitudetype == \"ml\":\n",
    "                            if 3.5 <= x <= 7:\n",
    "                                magnitude = 2.29155 + 0.02207 * x + 0.09782 * x**2\n",
    "                            elif x < 3.5:\n",
    "                                magnitude = 1.09329 + 0.70679 * x\n",
    "                            else:\n",
    "                                print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                        elif magnitudetype == \"md\":\n",
    "                            if 3.5 <= x <= 7:\n",
    "                                magnitude = 2.34570 + 0.00448 * x + 0.10308 * x**2\n",
    "                            elif x < 3.5:\n",
    "                                magnitude = 1.08291 + 0.72608 * x\n",
    "                            else:\n",
    "                                print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading nil_webmt.csv or processing event {event}: {e}\")\n",
    "                    print(f\"Assuming mathematical conversion for {magnitudetype} on {event}\")\n",
    "                    \n",
    "                    x = magnitude\n",
    "                    if magnitudetype == \"ml\":\n",
    "                        if 3.5 <= x <= 7:\n",
    "                            magnitude = 2.29155 + 0.02207 * x + 0.09782 * x**2\n",
    "                        elif x < 3.5:\n",
    "                            magnitude = 1.09329 + 0.70679 * x\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                    elif magnitudetype == \"md\":\n",
    "                        if 3.5 <= x <= 7:\n",
    "                            magnitude = 2.34570 + 0.00448 * x + 0.10308 * x**2\n",
    "                        elif x < 3.5:\n",
    "                            magnitude = 1.08291 + 0.72608 * x\n",
    "                        else:\n",
    "                            print(f\"No magnitude adjustment could be made for {event}\")\n",
    "                    else:\n",
    "                        print(f\"No magnitude adjustment could be made for {event}\")\n",
    "\n",
    "            ### Define Rupture Context\n",
    "            \n",
    "            rctx = RuptureContext()\n",
    "            rctx.mag = magnitude\n",
    "        \n",
    "            #try to find focal mechanism to add rake and dip\n",
    "            try:\n",
    "                mech = cat.focal_mechanisms[0]\n",
    "                nop = mech.nodal_planes['nodal_plane_1']\n",
    "                rctx.rake = nop['rake']\n",
    "                rctx.dip = nop['dip']\n",
    "                print(f'Found a focal mechanism for {magnitude} {event}')\n",
    "            except:\n",
    "                rctx.rake = 0\n",
    "                rctx.dip = 45\n",
    "                print(f'No focal mechanism available for {magnitude} {event}')\n",
    "                \n",
    "            # assume square ruptures with identical stress drop for every event\n",
    "            moment = 10 ** (1.5*(magnitude+10.7))\n",
    "            moment = moment * 10**-7\n",
    "            stressdrop = 3*10**6 #Pa\n",
    "            L = ((3*np.pi*moment)/(16*stressdrop))**(1/3) #m\n",
    "            L = L/1000 #km \n",
    "            rctx.ztor = depth-L/2 #from hypo to edge of square\n",
    "            rctx.width = L #length of square\n",
    "            rctx.hypo_depth = depth\n",
    "            \n",
    "            ### Define Distance Context (assume fault-perpendicular sites)\n",
    "            \n",
    "            dctx = DistancesContext()\n",
    "            rjbs = df['JoynerBooreDistance'] # rjb values in km\n",
    "            rrups = df['RuptureDistance'] # calculated rupture distance for a vertical fault\n",
    "            npts = rrups.size # number of distance points\n",
    "            dctx.rrup = rrups # Closest distance to rupture surface\n",
    "            dctx.rjb = rjbs # Distance to ruptureâ€™s surface projection\n",
    "            dctx.rx = np.zeros(npts) # Perpendicular distance to rupture top edge projection\n",
    "            dctx.ry0 = np.zeros(npts) # Horizontal distance off the end of the rupture measured parallel to strike \n",
    "                                     #  (essentially, how far a location is \"along the fault\" from the rupture termination point)\n",
    "            \n",
    "            distancestostore = df['RuptureDistance'].tolist()\n",
    "            totaldistances.extend(distancestostore)\n",
    "            \n",
    "            ### Site context - seems to now need to be from a \"site collection\", which seems to be a pandas dataframe.\n",
    "            \n",
    "            #    value to predict - if you're doing it in array form (many records for a single earthquake), then make these arrays.\n",
    "            # Prepare results\n",
    "            vs30_values = []\n",
    "            measured_flags = []\n",
    "        \n",
    "            #Find Vs30 measurements from USGS raster\n",
    "            # Iterate over stations\n",
    "            for _, station in df.iterrows():\n",
    "                lat = station['StationLatitude']\n",
    "                lon = station['StationLongitude']\n",
    "                # Query tree for neighbors within the threshold\n",
    "                distances, indices = tree.query([[lat, lon]], k=1, distance_upper_bound=threshold_deg)\n",
    "                index = indices[0]\n",
    "                if index != tree.n:  # Found a valid neighbor\n",
    "                    vs30 = vs30_df.iloc[index]['Vs30']\n",
    "                    measured = True\n",
    "                else:\n",
    "                    # Fallback to raster\n",
    "                    row, col = raster.index(lon, lat)\n",
    "                    if 0 <= row < raster_array.shape[0] and 0 <= col < raster_array.shape[1]:\n",
    "                        vs30 = raster_array[row, col]\n",
    "                    else:\n",
    "                        vs30 = np.nan\n",
    "                    measured = False\n",
    "                vs30_values.append(vs30)\n",
    "                measured_flags.append(measured)\n",
    "            \n",
    "            # Add columns to stations DataFrame\n",
    "            df['Vs30'] = vs30_values\n",
    "            df['measured'] = measured_flags\n",
    "            sitecol_dict = {'sids': np.arange(1, npts + 1), # site id\n",
    "                            'vs30': vs30_values, # Vs30 value in m/s\n",
    "                            'vs30measured': df['measured'], # true or false, not sure how this is used\n",
    "                            'z1pt0': 50.0 + np.zeros(npts, dtype=float), # depth in m (not km) to 1.0 km/s horizon\n",
    "                            'z2pt5': np.nan + np.zeros(npts, dtype=float), # depth in km (not m) to 2.5 km/s horizon\n",
    "                            }\n",
    "            \"\"\"'vs30': 760.0 + np.zeros(npts, dtype=float), # Vs30 value in m/s\"\"\"\n",
    "            sitecollection = pd.DataFrame(sitecol_dict)\n",
    "            sctx = SitesContext(sitecol=sitecollection)   \n",
    "            \n",
    "            ### Calculate Ground Motions\n",
    "            \n",
    "            # define metrics\n",
    "            #IMT = imt.PGA() #IS THIS IN G OR %G\n",
    "            IMT = imt.PGV()\n",
    "            #IMT = imt.SA(period=0.1)\n",
    "            uncertaintytype = const.StdDev.TOTAL\n",
    "            \n",
    "            # instantiate GMMs\n",
    "            ASK14 = AbrahamsonEtAl2014()\n",
    "            BSSA14 = BooreEtAl2014()\n",
    "            CB14 = CampbellBozorgnia2014()\n",
    "            CY14 = ChiouYoungs2014()\n",
    "            \n",
    "            # calculate ln ground motions for each\n",
    "            ln_median_ask14, ln_sd_ask14 = ASK14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_bssa14, ln_sd_bssa14 = BSSA14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_cb14, ln_sd_cb14 = CB14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            ln_median_cy14, ln_sd_cy14 = CY14.get_mean_and_stddevs(sctx, rctx, dctx, IMT, [uncertaintytype])\n",
    "            \n",
    "            # average across all four for active crustal values\n",
    "            ln_median_ac14 = 0.25 * (ln_median_ask14 + ln_median_bssa14 + ln_median_cb14 + ln_median_cy14)\n",
    "            ln_sd_ac14 = 0.25 * (ln_sd_ask14[0] + ln_sd_bssa14[0] + ln_sd_cb14[0] + ln_sd_cy14[0]) # sds are in a list of length 1   \n",
    "        \n",
    "            ### Find Observed Data\n",
    "            \n",
    "            # gather observed ground motion metrics\n",
    "            OBSdrup = df['RuptureDistance']\n",
    "            OBSpga = df['PGA']*0.01 #%g\n",
    "            OBSpgv = df['PGV'] #cm/s\n",
    "            OBSsa = df['SA(T=1.0000, D=0.050)']\n",
    "        \n",
    "            ''' identify which metric is being worked with '''\n",
    "            ObservedMetric = OBSpgv\n",
    "            \n",
    "            # calculate residuals with estimated data\n",
    "            residuals = []\n",
    "            for i in range(len(ln_median_ac14)):\n",
    "                residual = (np.log(ObservedMetric[i])-(ln_median_ac14[i]))/ln_sd_ac14[i] #normalized log residual\n",
    "                residuals.append(residual)\n",
    "            residuals = np.array(residuals)\n",
    "            df['Residuals']= residuals\n",
    "            mean_residual = np.mean(residuals)\n",
    "            totalresiduals.append(mean_residual)\n",
    "            mean_residual = round(mean_residual, 4)\n",
    "            totalstationresiduals.extend(residuals)\n",
    "        \n",
    "            # Find SNR values\n",
    "            snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "            # Prepare list to store the minimum SNRs for each StationID\n",
    "            min_snrs = []\n",
    "            # Iterate through each StationID\n",
    "            for station_id in df['StationID']:\n",
    "                station_id = str(station_id)  # Make sure it's a string\n",
    "                # Update count for this StationID\n",
    "                station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "                # Find rows in snr_df where TraceID starts with the StationID\n",
    "                matches = snr_df[\n",
    "                    snr_df['TraceID'].astype(str).str.startswith(str(station_id)) &\n",
    "                    ~snr_df['TraceID'].astype(str).str.endswith('Z')\n",
    "                ]\n",
    "            \n",
    "                if not matches.empty:\n",
    "                    min_snr = matches['SNR(1)'].min()\n",
    "                    # Add the result to the original DataFrame\n",
    "                    min_snrs.append(min_snr)\n",
    "                    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "                \n",
    "                else:\n",
    "                    min_snr = float('0') # Or some default, like None or 0\n",
    "                    print('Did not find stationID in SNR csv')\n",
    "                \n",
    "            df['MinSNR'] = min_snrs\n",
    "                \n",
    "            ### Plot Model Results\n",
    "            \n",
    "            # figure setup\n",
    "            fig, axi = plt.subplots(figsize=(6,4))\n",
    "            axi.set_facecolor(\"gainsboro\")\n",
    "            # Create a colormap\n",
    "            cmap = plt.colormaps['Reds']\n",
    "            # Normalize the values to the range 0-1 for the colormap\n",
    "            #norm = plt.Normalize(vmin=0, vmax=1520)\n",
    "            #norm = plt.Normalize(vmin=0, vmax=df['MinSNR'].min())\n",
    "            norm = colors.LogNorm(vmin=0.27, vmax=df['MinSNR'].mean())\n",
    "            \n",
    "            # plot GMPE\n",
    "            ly1 = ln_median_ac14 - ln_sd_ac14\n",
    "            ly2 = ln_median_ac14 + ln_sd_ac14\n",
    "            axi.fill_between(x=rrups,y1=np.exp(ly1),y2=np.exp(ly2),color=\"r\",alpha=0.25,lw=2)\n",
    "            plt.scatter(OBSdrup, ObservedMetric, c=min_snrs, cmap=cmap, norm=norm, s=5, label=f\"{event} Observed Motions\", zorder=2)\n",
    "            #plt.scatter(rrups,np.exp(ln_median_ac14),c=vs30_values, cmap='viridis', s=3)\n",
    "            axi.loglog(rrups,np.exp(ln_median_ac14),\"-r\",lw=3,label=\"Active Crustal GMM ($V_{S30}$=760m/s)\", zorder=0)\n",
    "            #axi.fill_between(x=rrups,y1=np.exp(ly1)+0.01,y2=np.exp(ly2)+0.05,color=\"b\",alpha=0.25,lw=2)\n",
    "            #axi.loglog(rrups,np.exp(ln_median_ac14)+0.05,\"-b\",lw=3,label=\"Hypothetical Updated GMM\", zorder=1)\n",
    "            \n",
    "            # formatting\n",
    "            plt.title(f'M{magnitude} {event}: residual={mean_residual}')\n",
    "            axi.set_xlabel(\"$R_{rup}$ [km]\",fontsize=16)\n",
    "            if ObservedMetric.equals(OBSpgv):\n",
    "                axi.set_ylabel(\"PGV [cm/s]\",fontsize=16)\n",
    "            if ObservedMetric.equals(OBSpga):\n",
    "                axi.set_ylabel(\"PGA [%g]\", fontsize=16)    ###Is something wrong with PGA units? Both should be %g but off by 100x, maybe predictions are just g\n",
    "            if ObservedMetric.equals(OBSsa):\n",
    "                axi.set_ylabel(\"SA (T=1.0000, D=0.050)[add units]\", fontsize=12)\n",
    "            axi.grid(lw=0.3)\n",
    "            axi.legend(loc=\"upper right\",fontsize=11)\n",
    "            axi.set_xlim(9,320)\n",
    "            plt.xscale('log')\n",
    "            plt.yscale('log')\n",
    "            plt.colorbar(label='SNR')\n",
    "        \n",
    "            pdf_pages.savefig(fig)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            print('Outside Magnitude Range')\n",
    "    else:\n",
    "        print('Outside Date Range')\n",
    "# Save the figure with a specified DPI\n",
    "#plt.savefig(\"/OpenquakeFigures/ProposalGMMFigure.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# show all gmm figures\n",
    "#plt.show()\n",
    "pdf_pages.close()\n",
    "\n",
    "#plot residual distribution\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].hist(totalresiduals, bins=20, edgecolor = \"black\")\n",
    "axes[0].set_title('Residuals Histogram (20 Bins)')\n",
    "axes[0].set_xlabel('Mean Residual Difference')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "M = math.ceil(max(totalresiduals))\n",
    "m = math.floor(min(totalresiduals))\n",
    "binwidth = 1\n",
    "numberbins = np.arange(m, M + binwidth, binwidth)\n",
    "\n",
    "axes[1].hist(totalresiduals, bins=numberbins, edgecolor = \"black\")\n",
    "axes[1].set_title(f'Residuals Histogram ({len(numberbins)} Bins Integers)')\n",
    "axes[1].set_xlabel('Mean Residual Difference')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.savefig(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/ResidualsHistograms.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "snr_summary_df = pd.DataFrame(station_snr_records)\n",
    "# Compute mean of minimum SNRs per station\n",
    "mean_snr_per_station = snr_summary_df.groupby('StationID')['MinSNR'].mean().reset_index()\n",
    "counts_df = pd.DataFrame(list(station_counts.items()), columns=['StationID', 'Count'])\n",
    "# Merge counts into the summary\n",
    "mean_snr_per_station = mean_snr_per_station.merge(counts_df, on='StationID')\n",
    "mean_snr_per_station.rename(columns={'MinSNR': 'MeanMinSNR'}, inplace=True)\n",
    "mean_snr_per_station = mean_snr_per_station.sort_values(by='MeanMinSNR', ascending=True)\n",
    "# Print or save\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(mean_snr_per_station)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "axes[0].scatter(totalmagnitudes, totalresiduals, s=2)\n",
    "axes[0].set_title('Magnitude vs Event Residual')\n",
    "axes[0].set_xlabel('Magnitude')\n",
    "axes[0].set_ylabel('Mean Residual')\n",
    "\n",
    "axes[1].scatter(totaldistances, totalstationresiduals, s=1)\n",
    "axes[1].set_title('Distance vs Station Residual')\n",
    "axes[1].set_xlabel('Rrup (km)')\n",
    "axes[1].set_ylabel('Station Residual Difference')\n",
    "plt.savefig(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/maganddistwithresiduals.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        eventID  NumRows\n",
      "117  nn00278760        1\n",
      "111  nn00278758        1\n",
      "106  nn00278756        1\n",
      "393  nn00888664        1\n",
      "300  nn00278754        1\n",
      "299  nn00278762        2\n",
      "410  nc71126849        3\n",
      "156  nn00185054        3\n",
      "318  nn00001802        4\n",
      "180  nn00487790        4\n",
      "127  nn00002400        5\n",
      "406  nn00345789        5\n",
      "401  nc21152723        5\n",
      "370  nn00208650        5\n",
      "158  nn00191064        6\n",
      "155  nc21169351        7\n",
      "61   nc72796075        7\n",
      "198  nn00001757        7\n",
      "125  nn00436637        8\n",
      "224  nn00651540        8\n",
      "337  nn00278764        8\n",
      "46   nn00059455        9\n",
      "227  nc21105736       10\n",
      "9    nc21227205       10\n",
      "448  nn00167838       10\n",
      "313  nc21142104       11\n",
      "414  nc21126268       11\n",
      "261  nc21099303       12\n",
      "409  nc21130131       12\n",
      "433  nn00020076       12\n",
      "427  nn00021354       12\n",
      "274  nn00178631       13\n",
      "270  nc21254752       13\n",
      "141  nc21086071       13\n",
      "398  nn00279090       14\n",
      "269  nn00106671       15\n",
      "129  nc21267688       15\n",
      "454  nn00518396       15\n",
      "371  nn00123032       16\n",
      "445  nn00057973       16\n",
      "69   nn00487602       16\n",
      "369  nn00107061       17\n",
      "184  nn00059477       17\n",
      "114  nn00278767       17\n",
      "243  nn00138182       18\n",
      "230  nn00888575       18\n",
      "42   nn00030298       18\n",
      "262  nn00067198       19\n",
      "188  nn00048730       20\n",
      "77   nn00278766       20\n",
      "192  nn00343959       21\n",
      "359  nc73855405       21\n",
      "296  nn00066198       21\n",
      "112  nn00888610       21\n",
      "436  nn00518397       22\n",
      "278  nn00105633       23\n",
      "307  nn00436688       23\n",
      "68   nc21307453       24\n",
      "233  nn00130120       24\n",
      "10   nn00044183       24\n",
      "134  nn00038991       24\n",
      "266  nn00242529       24\n",
      "333  nn00104454       25\n",
      "56   nn00067737       25\n",
      "137  nn00516927       25\n",
      "282  nn00295940       25\n",
      "457  nn00580233       26\n",
      "424  nn00248430       27\n",
      "174  nc21440508       27\n",
      "435  nc21370408       28\n",
      "150  nn00487689       29\n",
      "43   nc71354895       29\n",
      "432  nn00122461       30\n",
      "27   nn00137502       30\n",
      "348  nc21368395       30\n",
      "403  nn00278771       32\n",
      "242  nn00421583       32\n",
      "17   nn00279138       32\n",
      "21   nn00888364       33\n",
      "463  nn00337688       33\n",
      "276  nn00282304       33\n",
      "373  nn00524671       34\n",
      "66   nn00435804       34\n",
      "383  nn00527268       34\n",
      "413  nn00436632       35\n",
      "339  nn00137417       35\n",
      "382  nn00586270       35\n",
      "321  nc21370353       36\n",
      "306  nn00313976       36\n",
      "110  nn00528459       36\n",
      "390  nn00075738       37\n",
      "162  nn00433910       38\n",
      "49   nc21081757       38\n",
      "387  nc71451855       38\n",
      "251  nn00341416       38\n",
      "315  nn00494672       38\n",
      "258  nn00253832       40\n",
      "130  nn00307350       40\n",
      "452  nn00264665       40\n",
      "170  nn00503403       41\n",
      "349  nn00032442       41\n",
      "163  nn00436230       41\n",
      "60   nn00523705       41\n",
      "83   nc21452289       41\n",
      "171  nc21457914       42\n",
      "286  nn00243658       42\n",
      "305  nn00117417       43\n",
      "458  nn00512464       43\n",
      "254  nn00307267       44\n",
      "35   nn00395207       44\n",
      "340  nc21370352       45\n",
      "404  nc71123164       45\n",
      "221  nc72372641       45\n",
      "324  nn00436681       47\n",
      "228  nn00888749       47\n",
      "143  nn00459165       47\n",
      "360  nc72112115       49\n",
      "18   nn00390140       49\n",
      "364  nn00432465       49\n",
      "152  nc72822206       49\n",
      "344  nc72713711       50\n",
      "63   nc71958445       50\n",
      "310  nn00494644       50\n",
      "399  nn00436659       50\n",
      "12   nn00523704       50\n",
      "356  nn00494499       50\n",
      "126  nn00264545       50\n",
      "113  nn00286275       51\n",
      "411  nn00380485       53\n",
      "451  nn00243275       54\n",
      "205  nn00321527       54\n",
      "91   nn00243576       54\n",
      "460  nn00366485       54\n",
      "214  nn00479246       55\n",
      "400  nn00606357       55\n",
      "92   nn00243582       55\n",
      "277  nn00751484       55\n",
      "351  nn00523675       55\n",
      "132  nn00243596       55\n",
      "100  nc72269026       56\n",
      "166  nn00523690       56\n",
      "103  nn00523682       56\n",
      "441  nc21469100       57\n",
      "223  nn00322758       57\n",
      "197  nc71956270       57\n",
      "275  nc72241101       57\n",
      "232  nc71475591       57\n",
      "271  nn00258424       58\n",
      "272  nc73027576       58\n",
      "53   nn00241158       59\n",
      "102  nn00553959       59\n",
      "415  nn00279659       60\n",
      "234  nn00617945       60\n",
      "76   nn00430713       60\n",
      "434  nn00429763       60\n",
      "263  nc71522186       60\n",
      "200  nn00243684       61\n",
      "241  nc71691016       61\n",
      "329  nn00522707       61\n",
      "67   nn00279614       61\n",
      "389  nn00243763       62\n",
      "34   nn00380513       62\n",
      "377  nn00273168       62\n",
      "429  nc72193415       62\n",
      "464  nn00888571       63\n",
      "396  nc72321601       63\n",
      "11   nc73554390       63\n",
      "374  nc72822266       63\n",
      "447  nn00252300       64\n",
      "288  nn00516642       64\n",
      "331  nn00833739       64\n",
      "86   nn00350873       64\n",
      "73   nn00793883       64\n",
      "325  nn00178326       65\n",
      "267  nc71680395       65\n",
      "173  nn00245667       65\n",
      "16   nc71761975       65\n",
      "293  nn00169297       65\n",
      "36   nn00239224       66\n",
      "196  nn00821396       66\n",
      "280  nc73036051       66\n",
      "146  nc73193831       66\n",
      "54   nn00382219       66\n",
      "120  nc72241091       66\n",
      "472  nn00241185       66\n",
      "308  nc73566741       67\n",
      "392  nn00523693       67\n",
      "41   nc73524686       67\n",
      "195  nn00888573       67\n",
      "421  nn00451213       67\n",
      "245  nc72973441       67\n",
      "216  nn00385016       67\n",
      "381  nn00165483       68\n",
      "328  nn00307385       68\n",
      "194  nn00751469       68\n",
      "430  nn00751490       68\n",
      "439  nn00415768       69\n",
      "345  nn00380292       69\n",
      "236  nn00540403       69\n",
      "116  nn00321478       69\n",
      "37   nn00845098       69\n",
      "408  nc72983196       69\n",
      "412  nn00321469       69\n",
      "372  nn00243550       70\n",
      "317  nn00414380       70\n",
      "154  nn00263795       70\n",
      "240  nc73308186       70\n",
      "199  nn00888587       70\n",
      "38   nc73938276       70\n",
      "290  nn00256173       71\n",
      "40   nn00242381       71\n",
      "28   nn00720284       71\n",
      "160  nn00239115       71\n",
      "32   nn00751474       71\n",
      "423  nc40226537       72\n",
      "229  nn00256508       72\n",
      "442  nn00618549       72\n",
      "425  nn00241743       72\n",
      "247  nn00380529       73\n",
      "202  nc73152811       73\n",
      "235  nn00242187       73\n",
      "289  nn00759625       73\n",
      "30   nn00242546       73\n",
      "459  nn00321514       74\n",
      "388  nn00242483       74\n",
      "402  nn00659830       74\n",
      "238  nn00238353       74\n",
      "311  nc73558875       74\n",
      "473  nn00242532       74\n",
      "109  nn00242631       74\n",
      "94   nn00243788       74\n",
      "469  nn00668094       75\n",
      "72   nn00242405       75\n",
      "334  nn00274506       75\n",
      "330  nn00251646       75\n",
      "183  nn00242539       75\n",
      "255  nn00242715       75\n",
      "0    nc51198230       75\n",
      "58   nc21509519       76\n",
      "71   nn00242035       76\n",
      "98   nn00241226       76\n",
      "101  nn00242495       76\n",
      "62   nc71502540       76\n",
      "407  nn00638434       76\n",
      "161  nn00681228       76\n",
      "260  nn00242382       76\n",
      "385  nn00888600       76\n",
      "252  nn00169321       76\n",
      "419  nn00241404       77\n",
      "303  nn00487664       77\n",
      "82   nn00888618       77\n",
      "326  nn00348842       77\n",
      "250  nn00242588       77\n",
      "346  nn00241813       77\n",
      "338  nc73051211       77\n",
      "151  nn00163749       77\n",
      "74   nc73193846       78\n",
      "291  nn00250426       78\n",
      "164  nn00244121       78\n",
      "88   nn00231936       78\n",
      "466  nn00421303       78\n",
      "319  nn00229399       78\n",
      "431  nn00239462       78\n",
      "48   nc71607645       78\n",
      "249  nn00178608       78\n",
      "426  nn00244231       78\n",
      "97   nn00245216       79\n",
      "467  nn00242158       79\n",
      "357  nn00271808       80\n",
      "31   nn00341413       80\n",
      "335  nn00244367       80\n",
      "417  nn00670948       80\n",
      "159  nn00244770       80\n",
      "362  nn00243901       81\n",
      "44   nc72822101       81\n",
      "218  nn00272995       81\n",
      "95   nc72822081       81\n",
      "470  nn00243413       81\n",
      "13   nn00242719       82\n",
      "23   nn00415325       82\n",
      "203  nc71698320       82\n",
      "284  nc72822151       82\n",
      "365  nc72488831       82\n",
      "418  nc73554405       82\n",
      "210  nn00240039       83\n",
      "375  nn00241238       83\n",
      "268  nn00241152       83\n",
      "438  nn00283061       84\n",
      "8    nn00244218       84\n",
      "2    nn00242585       84\n",
      "185  nn00629667       84\n",
      "378  nn00243352       84\n",
      "207  nn00242563       84\n",
      "391  nn00242477       85\n",
      "7    nn00888595       85\n",
      "246  nn00243008       86\n",
      "343  nc73566776       86\n",
      "4    nn00245339       86\n",
      "104  nn00244503       87\n",
      "462  nn00888585       87\n",
      "81   nn00243344       87\n",
      "449  nc73509325       88\n",
      "187  nn00246969       88\n",
      "39   nn00237524       88\n",
      "428  nn00874356       88\n",
      "273  nn00279151       88\n",
      "90   nc71660896       88\n",
      "332  nn00249024       89\n",
      "80   nn00242296       89\n",
      "248  nn00244029       90\n",
      "191  nn00889009       90\n",
      "186  nc72296116       90\n",
      "15   nn00283280       91\n",
      "59   nn00714016       91\n",
      "121  nn00162499       91\n",
      "222  nn00248665       91\n",
      "55   nc73506345       91\n",
      "24   nn00249902       92\n",
      "312  nn00343054       92\n",
      "259  nn00879860       92\n",
      "206  nc21368407       93\n",
      "167  nc73346460       93\n",
      "212  nc73109741       93\n",
      "142  nn00346705       93\n",
      "342  nn00265044       94\n",
      "181  nn00255746       94\n",
      "201  nc71504090       94\n",
      "405  nc72428161       95\n",
      "244  nc51212688       95\n",
      "301  nn00243347       95\n",
      "20   nn00867360       95\n",
      "25   nn00873356       96\n",
      "93   nc51181693       96\n",
      "450  nn00638386       96\n",
      "302  nn00242605       96\n",
      "136  nn00242215       98\n",
      "455  nn00242561       98\n",
      "165  nc73566756       98\n",
      "135  nn00245454       99\n",
      "397  nn00245800       99\n",
      "177  nn00247598      100\n",
      "172  nn00888804      100\n",
      "354  nn00250198      100\n",
      "309  nn00872355      100\n",
      "366  nn00889316      101\n",
      "131  nn00247350      101\n",
      "257  nn00248821      102\n",
      "208  nn00888784      103\n",
      "85   nn00248760      103\n",
      "239  nc73723705      104\n",
      "355  nn00888685      105\n",
      "379  nn00888636      106\n",
      "33   nc73544965      106\n",
      "211  nn00242164      106\n",
      "322  nn00231198      106\n",
      "440  nn00242157      107\n",
      "138  nc75090886      108\n",
      "64   nn00888826      108\n",
      "168  nn00860674      108\n",
      "6    nc73428961      108\n",
      "182  nn00860991      108\n",
      "128  nn00243152      110\n",
      "75   nc73407201      110\n",
      "465  nn00249127      110\n",
      "380  nn00888863      111\n",
      "52   nn00889047      111\n",
      "420  nc73343145      111\n",
      "3    nc72822191      112\n",
      "279  nn00889487      113\n",
      "115  nn00888280      115\n",
      "87   nn00874233      115\n",
      "153  nn00889181      116\n",
      "226  nn00889465      116\n",
      "217  nc73578975      119\n",
      "384  nn00248740      119\n",
      "281  nc75037262      119\n",
      "169  nn00712041      120\n",
      "124  nn00863066      121\n",
      "105  nc73352670      121\n",
      "443  nn00889068      122\n",
      "144  nn00523696      123\n",
      "298  nn00190934      125\n",
      "22   nn00687042      126\n",
      "327  nn00889155      126\n",
      "108  nc73597286      126\n",
      "107  nn00888621      126\n",
      "231  nn00888581      127\n",
      "341  nc72822076      127\n",
      "314  nn00380267      128\n",
      "264  nn00870310      131\n",
      "26   nn00888938      132\n",
      "352  nn00889139      133\n",
      "444  nn00888972      134\n",
      "422  nn00709183      135\n",
      "395  nn00787091      137\n",
      "175  nn00861551      137\n",
      "358  nn00526746      137\n",
      "145  nn00874424      138\n",
      "70   nn00861783      138\n",
      "204  nn00778922      139\n",
      "65   nn00864692      139\n",
      "99   nn00889754      139\n",
      "285  nn00888760      139\n",
      "119  nc73554400      142\n",
      "320  nn00862754      143\n",
      "253  nn00888591      146\n",
      "225  nn00888588      148\n",
      "353  nn00670931      148\n",
      "237  nc73296890      148\n",
      "122  nn00888604      149\n",
      "376  nn00889115      150\n",
      "283  nn00162500      151\n",
      "1    nc73563910      151\n",
      "29   nc73502910      151\n",
      "265  nn00888959      152\n",
      "140  nn00889312      153\n",
      "287  nc21465580      153\n",
      "347  nn00860806      153\n",
      "123  nn00861503      153\n",
      "178  nn00385392      154\n",
      "394  nn00889584      154\n",
      "50   nn00889423      155\n",
      "89   nn00888643      156\n",
      "367  nn00888654      157\n",
      "368  nn00867659      159\n",
      "416  nc73643690      162\n",
      "213  nn00889400      162\n",
      "256  nn00242527      162\n",
      "220  nn00849284      164\n",
      "456  nn00889492      164\n",
      "139  nn00861791      164\n",
      "350  nn00873424      165\n",
      "471  nc73056106      166\n",
      "45   nn00870545      167\n",
      "176  nc73855400      168\n",
      "468  nc73566621      171\n",
      "304  nn00719846      171\n",
      "14   nn00888707      171\n",
      "361  nc72822096      172\n",
      "148  nn00861737      173\n",
      "78   nn00888611      176\n",
      "336  nn00888613      176\n",
      "215  nc75037447      177\n",
      "157  nc73566751      178\n",
      "19   nn00889414      181\n",
      "295  nn00861727      182\n",
      "179  nn00860645      183\n",
      "47   nn00867368      183\n",
      "386  nn00888607      184\n",
      "57   nn00889415      184\n",
      "189  nc73078235      185\n",
      "5    nn00867596      188\n",
      "461  nn00867575      189\n",
      "446  nn00889608      191\n",
      "437  nc75037427      191\n",
      "453  nc73559260      193\n",
      "79   nn00888275      196\n",
      "147  nn00888661      199\n",
      "51   nn00848972      200\n",
      "323  nn00819000      200\n",
      "193  nn00888580      206\n",
      "219  nn00870301      207\n",
      "292  nc73815951      210\n",
      "118  nc73554845      211\n",
      "149  nn00872549      225\n",
      "96   nc73352615      229\n",
      "294  nc73563860      230\n",
      "363  nn00719663      254\n",
      "133  nc74000481      256\n",
      "297  nn00860639      261\n",
      "190  nc73554385      265\n",
      "84   nc73566711      267\n",
      "209  nc73559265      282\n",
      "316  nc73938166      284\n"
     ]
    }
   ],
   "source": [
    "#COUNT RECORDINGS PER EVENT\n",
    "\n",
    "# Load event IDs from CSV\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "event_ids = eventdata['eventID']\n",
    "\n",
    "# Initialize list to store results\n",
    "row_counts = []\n",
    "\n",
    "# Loop through each event\n",
    "for event in event_ids:\n",
    "    base_dir = f\"/Users/carterdills/Documents/Reno1/{event}/data\"\n",
    "    filename = f\"{event}_default_metrics_rotd(percentile=50.0).csv\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            row_count = len(df)  # includes data rows only\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "            row_count = -1  # flag for read error\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        row_count = 0  # file missing\n",
    "\n",
    "    # Store result\n",
    "    row_counts.append({'eventID': event, 'NumRows': row_count})\n",
    "\n",
    "# Save results to CSV\n",
    "counts_df = pd.DataFrame(row_counts)\n",
    "counts_df = counts_df.sort_values(by='NumRows', ascending=True)\n",
    "\n",
    "# Print summary\n",
    "# Set display options to show all rows, columns, and full column width\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None) # Or a large integer for max characters\n",
    "print(counts_df)\n",
    "# Optionally, reset display options to default\n",
    "pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNR Ratio Assembly\n",
    "\n",
    "#load basic event characteristic data (query2)\n",
    "eventdata = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "#eventdata = pd.read_csv(\"ParkerButteTestCSV.csv\")\n",
    "#eventdata = pd.read_csv(\"EventsPracticeSubset.csv\")\n",
    "\n",
    "for _, row in eventdata.iterrows():\n",
    "    event = row['eventID']\n",
    "    # Find SNR values\n",
    "            snr_df = pd.read_csv(f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\")\n",
    "            # Prepare list to store the minimum SNRs for each StationID\n",
    "            min_snrs = []\n",
    "            # Iterate through each StationID\n",
    "            for station_id in df['StationID']:\n",
    "                station_id = str(station_id)  # Make sure it's a string\n",
    "                # Update count for this StationID\n",
    "                station_counts[station_id] = station_counts.get(station_id, 0) + 1\n",
    "                # Find rows in snr_df where TraceID starts with the StationID\n",
    "                matches = snr_df[\n",
    "                    snr_df['TraceID'].astype(str).str.startswith(str(station_id)) &\n",
    "                    ~snr_df['TraceID'].astype(str).str.endswith('Z')\n",
    "                ]\n",
    "            \n",
    "                if not matches.empty:\n",
    "                    min_snr = matches['SNR(1)'].min()\n",
    "                    # Add the result to the original DataFrame\n",
    "                    min_snrs.append(min_snr)\n",
    "                    station_snr_records.append({'StationID': station_id, 'MinSNR': min_snr})\n",
    "                \n",
    "                else:\n",
    "                    min_snr = float('0') # Or some default, like None or 0\n",
    "                    print('Did not find stationID in SNR csv')\n",
    "                \n",
    "            df['MinSNR'] = min_snrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary Statistics ---\n",
      "Total recordings analyzed: 119678\n",
      "Unique events analyzed: 474\n",
      "Events processed: 474\n",
      "Missing SNR files: 0\n",
      "Total events listed: 474\n",
      "Saved heatmaps to SNR_heatmaps.pdf âœ…\n"
     ]
    }
   ],
   "source": [
    "# Load event list\n",
    "events_df = pd.read_csv(\"ListofCompletedEvents.csv\")\n",
    "event_ids = events_df['eventID'].dropna().astype(str).tolist()\n",
    "\n",
    "# Thresholds and bins\n",
    "thresholds = [1, 2, 3, 4, 5, 8, 10, 20]\n",
    "magnitude_bins = [(2.5, 3), (3, 4), (4, 5), (5, 6)]\n",
    "\n",
    "# Storage\n",
    "all_data = []  # Store all SNR rows with their magnitude and EarthquakeId\n",
    "\n",
    "# Stats\n",
    "missing_files = 0\n",
    "events_processed = 0\n",
    "\n",
    "for event in event_ids:\n",
    "    snr_path = f\"/Users/carterdills/Documents/Reno1/{event}/data/{event}_default_snr.csv\"\n",
    "\n",
    "    if not os.path.exists(snr_path):\n",
    "        print(f\"Missing file: {event}\")\n",
    "        missing_files += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(snr_path)\n",
    "        if 'EarthquakeMagnitude' not in df.columns:\n",
    "            print(f\"Missing magnitude column in {event}\")\n",
    "            continue\n",
    "\n",
    "        snr_columns = df.columns[-16:]\n",
    "        mag = df['EarthquakeMagnitude'].values[0]\n",
    "        snr_df = df[snr_columns].copy()\n",
    "        snr_df['Magnitude'] = mag\n",
    "        snr_df['EarthquakeId'] = df['EarthquakeId']\n",
    "\n",
    "        all_data.append(snr_df)\n",
    "        events_processed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {event}: {e}\")\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Frequencies from SNR columns\n",
    "frequency_labels = combined_df.columns[:-2].tolist()  # exclude 'Magnitude' and 'EarthquakeId'\n",
    "\n",
    "# Function to calculate failure matrix\n",
    "def calculate_failure_matrix(data, thresholds, freq_labels):\n",
    "    failure_counts = pd.DataFrame(0, index=freq_labels, columns=thresholds)\n",
    "    total = data.shape[0]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        failures = (data[freq_labels] < threshold).sum()\n",
    "        failure_counts[threshold] = failures\n",
    "\n",
    "    return failure_counts, total\n",
    "\n",
    "# Function to plot heatmap with annotations\n",
    "def plot_heatmap(failure_counts, total, n_events, title_prefix):\n",
    "    heatmap_data = failure_counts.astype(int).T\n",
    "    annotations = heatmap_data.copy().astype(str)\n",
    "\n",
    "    for t in heatmap_data.index:\n",
    "        for f in heatmap_data.columns:\n",
    "            count = heatmap_data.at[t, f]\n",
    "            percent = (count / total) * 100 if total > 0 else 0\n",
    "            annotations.at[t, f] = f\"{count} ({percent:.1f}%)\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        heatmap_data.T,\n",
    "        annot=annotations.T,\n",
    "        annot_kws={\"size\": 9},\n",
    "        fmt=\"\",\n",
    "        cmap=\"Reds\",\n",
    "        cbar_kws={'label': 'Count of Failing Recordings'}\n",
    "    )\n",
    "    plt.title(f\"{title_prefix} [{n_events} Events, {total} Recordings]\")\n",
    "    plt.xlabel(\"SNR Threshold\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Save all plots into one PDF\n",
    "with PdfPages(\"/Users/carterdills/openquake/lib/python3.11/site-packages/OpenquakeFigures/SNR_heatmaps.pdf\") as pdf:\n",
    "    # All data\n",
    "    failures_all, total_all = calculate_failure_matrix(combined_df, thresholds, frequency_labels)\n",
    "    n_events_all = combined_df['EarthquakeId'].nunique()\n",
    "    plot_heatmap(failures_all, total_all, n_events_all, \"SNR Failures (All Magnitudes)\")\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "    # Each magnitude bin\n",
    "    for lower, upper in magnitude_bins:\n",
    "        bin_df = combined_df[(combined_df['Magnitude'] >= lower) & (combined_df['Magnitude'] < upper)]\n",
    "        if bin_df.empty:\n",
    "            print(f\"No data for magnitude range {lower}â€“{upper}\")\n",
    "            continue\n",
    "\n",
    "        failures_bin, total_bin = calculate_failure_matrix(bin_df, thresholds, frequency_labels)\n",
    "        n_events_bin = bin_df['EarthquakeId'].nunique()\n",
    "        plot_heatmap(failures_bin, total_bin, n_events_bin, f\"SNR Failures (Magnitude {lower}â€“{upper})\")\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "# Summary output\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "print(f\"Total recordings analyzed: {combined_df.shape[0]}\")\n",
    "print(f\"Unique events analyzed: {combined_df['EarthquakeId'].nunique()}\")\n",
    "print(f\"Events processed: {events_processed}\")\n",
    "print(f\"Missing SNR files: {missing_files}\")\n",
    "print(f\"Total events listed: {len(event_ids)}\")\n",
    "print(\"Saved heatmaps to SNR_heatmaps.pdf âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
